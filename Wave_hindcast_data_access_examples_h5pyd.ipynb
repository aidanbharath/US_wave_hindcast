{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NREL Wave Hindacast Data Access Example\n",
    "\n",
    "#### This notebook demonstrates basic usage of the National Renewable Energy Laboratory (NREL) West Coast Wave Hindcast dataset. The data is provided from Amazon Web Services using the HDF Group's Highly Scalable Data Service (HSDS).\n",
    "\n",
    "#### For this example to work, the packages necessary can be installed via pip:\n",
    "\n",
    "pip install -r requirements.txt\n",
    "\n",
    "#### Next you'll need to configure h5pyd to access the data on HSDS:\n",
    "\n",
    "hsconfigure\n",
    "\n",
    "#### and enter at the prompt:\n",
    "\n",
    "hs_endpoint = https://developer.nrel.gov/api/hsds   \n",
    "hs_username = None   \n",
    "hs_password = None   \n",
    "hs_api_key = 3K3JQbjZmWctY0xmIfSYvYgtIcM3CN0cb1Y2w9bf    \n",
    "\n",
    "#### The example API key here is for demonstation and is rate-limited per IP. To get your own API key, visit https://developer.nrel.gov/signup/\n",
    "\n",
    "#### You can also add the above contents to a configuration file at ~/.hscfg\n",
    "\n",
    "#### Finally, you can use Jupyter Notebook or Lab to view the example notebooks depending on your preference\n",
    "\n",
    "For example, to start the jupyter notebook server on your machine, type the following at a command prompt in the folder containing these files:\n",
    "\n",
    "    jupyter notebook\n",
    "\n",
    "This should start the server and open a browser window that contains these files, then open *this* ipython notebook, and get started!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5pyd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Usage\n",
    "\n",
    "The Wave Hindcast Dataset is provided in annual .h5 files and currently spans the Western Coastal Region of the Lower 48 from 1979-2010.\n",
    "\n",
    "Each year can be accessed from /nrel/us-wave/US_Wave_${year}.h5\n",
    "\n",
    "To open the desired year of Wave Hindcast data server endpoint, username, password is found via a config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waves = h5pyd.File(f'/nrel/us-wave/US_Wave_1990.h5','r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(waves) # list of base attributes of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "Each dataset quantity:\n",
    "\n",
    "- directionality_coefficient\n",
    "- energy_period\n",
    "- maximum_energy_direction\n",
    "- mean_absolute_period\n",
    "- mean_zero-crossing_period\n",
    "- omni-directional_wave_power\n",
    "- peak_period\n",
    "- significant_wave_height\n",
    "- spectral_width\n",
    "- water_depth\n",
    "\n",
    "is structured as ('time_index','coordinate')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shapes of datasets\n",
    "waves['significant_water_height'].shape # (time_index, coordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the available dataset variable names within the dataset\n",
    "print(list(waves['coordinates'].attrs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locational information is stored in either 'meta' or 'coordinates'\n",
    "\n",
    "meta = pd.DataFrame(waves['meta'][:])\n",
    "meta.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Usage\n",
    "\n",
    "The following examples illustrate basic spatial and timeseries based slicing techniques for the West Coast Wave Hindcast dataset, along with simple statistical analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Slicing\n",
    "\n",
    "Basic description of timestamp extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract datetime index for datasets (Saved as binary)\n",
    "\n",
    "time_index = pd.to_datetime(waves['time_index'][:].astype(str))\n",
    "time_index, time_index.inferred_freq # Temporal resolution is 3 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Aug_7th_midnight = time_index.get_loc('1990-08-07 00:00:00') #precise data returns single index\n",
    "Aug = time_index.get_loc('1990-08') #Inprecise will return a range a values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'August 7th midnight: {time_index[Aug_7th_midnight]}')\n",
    "print(f'Month of August: {time_index[Aug]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping Data\n",
    "\n",
    "Basic methods to quickly plot data from the Hindcast spatially and temporally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spatial Plots\n",
    "\n",
    "Coordinate pair indices are used to slice the datasets, and are accessed through the 'coordinates' attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coord_slice = 100\n",
    "\n",
    "print(f'Available attributes: {dict(waves[\"coordinates\"].attrs)}')\n",
    "coords = waves['coordinates'][::coord_slice]\n",
    "print(f'Coordinate pairs: {coords}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a scatter plot of significant wave height values\n",
    "\n",
    "df = pd.DataFrame() # Combine data with coordinates in a DataFrame\n",
    "df['longitude'] = coords[:, 1]\n",
    "df['latitude'] = coords[:, 0]\n",
    "df['Hsig'] = waves['significant_water_height'][Aug_7th_midnight, ::coord_slice] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "df['Hsig'][df['Hsig']<0] = np.nan # There are negative values here Levi\n",
    "df.plot.scatter(x='longitude', y='latitude', c='Hsig',\n",
    "                colormap='viridis',\n",
    "                title=str(time_index[Aug_7th_midnight]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timeseries Plots\n",
    "\n",
    "Extracting a timeseries usually requires us to find a location of interest.      \n",
    "This can be done easially with KDTree, a function within scipy.spatial.   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import cKDTree\n",
    "\n",
    "tree = cKDTree(coords)\n",
    "def nearest_site(tree, lat_coord, lon_coord):\n",
    "    lat_lon = np.array([lat_coord, lon_coord])\n",
    "    dist, pos = tree.query(lat_lon)\n",
    "    return pos\n",
    "\n",
    "lat_coord, lon_coord = 39.6122, -125.318\n",
    "site_idx = nearest_site(tree, lat_coord, lon_coord)\n",
    "\n",
    "print(f'Site index: {site_idx}')\n",
    "print(f'Coordinates Searched: {lat_coord, lon_coord}')\n",
    "print(f'Coordinates of Nearest Point: {coords[site_idx]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can extract data at that location over a specified time range\n",
    "\n",
    "var = 'peak_period'\n",
    "\n",
    "dt = pd.DataFrame(waves[var][Aug,site_idx],index=time_index[Aug],columns=[var])\n",
    "dt.plot(y=var,title=f'Time sliced {var} Timeseries')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Statistics and Grouping\n",
    "\n",
    "Here is an example of grouping data to create quick plots of statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vars =['significant_water_height','peak_period'] \n",
    "variables = np.array([waves[Vars[0]][Aug,site_idx], waves[Vars[1]][Aug,site_idx]])\n",
    "\n",
    "ds = pd.DataFrame(variables.T,\n",
    "                    index=time_index[Aug],\n",
    "                    columns=Vars\n",
    "                )\n",
    "group = ds.groupby('peak_period')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the number of occurances of sea-states within peak period bins\n",
    "\n",
    "group.count().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the mean Hsig of the sea-states within peak period bins along with\n",
    "# the variance of those waveheights\n",
    "\n",
    "group.mean().plot(kind='bar',yerr=group.var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
